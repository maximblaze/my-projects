{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " \"HTR_research.ipynb\"",
      "provenance": [],
      "collapsed_sections": [
        "QSh2pmwBf1OP",
        "geqmorVigLf8",
        "Qd5tvjAugc8k",
        "6iDYfxj5kZJw",
        "6bcWt3oA8bOX"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QSh2pmwBf1OP"
      },
      "source": [
        "# Обзор литературы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geqmorVigLf8"
      },
      "source": [
        "## Обзор"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-tjV2rRf9jk"
      },
      "source": [
        "Распознавание и математический анализ внешней информации компьютером в наше время процесс популярный, являющийся, в сущности, основой научной деятельности, бизнеса, систем безопасности и прочих немаловажных элементов повседневности. От относительно простых схем детектирования геометрических структур и цветов человечество быстро перешло к сложным фигурам, комбинациям объектов и, конечно же, текстовым данным.\n",
        "\n",
        "Компьютерный анализ рукописных документов востребован теперь в широком спектре задач, в том числе при организации деятельности крупных банков и страховых компаний [5]. В основе процесса лежит растровое изображение текста, переданное в память через периферийное устройство, ради упрощения анализа и редактирования требующее дальнейшей машинной обработки, методы которой различны и развиваются с каждым днем.\n",
        "\n",
        "В основе своей эти методы делятся на два кластера: «онлайн» и «оффлайн» [1]. «Оффлайн» подразумевает распознавание статичных данных, уже переданных полноценно в память компьютера (чем мы и займемся), а «онлайн» это, скорее, процесс слежения за динамической генерацией текста человеком в реальном мире ручкой или стилусом, что несколько проще.\n",
        "\n",
        "Поскольку не все люди имеют хороший почерк, и не все камеры производят чистые сэмплы, производство «оффлайн» обработки начинается с попытки максимально улучшить качество изображений (резкость, контрастность, четкость) путем обработки. Таким образом удается избавиться от некоторого процента искажений, перекосов, разрывов, клякс и прочих нюансах рукописного текста.\n",
        "\n",
        "Итак, начнем с предобработки фотографий. Прежде всего, конечно, происходит изменение размеров изображений к одному шаблону для корректной работы алгоритмов. Далее специалисты советуют начать со сглаживания спектра цветов изображения и шумоподавления, после чего происходит приведение палитры сэмпла к черно-белой. Это увеличит контрастность границ символов и уменьшит проблемы с сегментацией, выделит элементы на фоне. Для этого применим инструментарий библиотек Python 'PIL', и 'cv2' [1].\n",
        "\n",
        "Далее мы переходим к этапу сегментации структурных элементов обработанного документа: предложений и слов. \n",
        "\n",
        "И вновь мы, прежде всего, столкнемся с проблемой неточностей и геометрических ошибок (изгибов, асимметрии и проч.) даже на начальном этапе рассмотрения абзацев. Горизонтальные линии в тексте (на которые \"нанизаны\" наши предложения) выделяются также разными методами:\n",
        "\n",
        "1. Top-down метод - метод, основанный на проецировании изображений на горизонтальные прямые [4].\n",
        "2. Bottom-up метод - отдельные элементы группируются на основе их геометрических качеств [4].\n",
        "3. Разбиение на \"базовые линии\" - основанный на методах оптимизации алгоритм стягивания крупных сегментов документа к воображаемой прямой [3].\n",
        "\n",
        "Поверх выделенных предложений применяется, соответственно, структурирование отдельных слов, проблемы и искажения которых схожи с проблемами предложений. \n",
        "\n",
        "Процесс сегментации слов целиком проводится также схожими методами:\n",
        "\n",
        "1. Bottom-up метод - аналогично выделению строк происходит группировка компонент по геометрическим свойствам [2].\n",
        "2. Нейросетевой метод - предполагает анализ признаковой картины сегментов текста и выявление отдельных слов при помощи моделирования [3]. \n",
        "\n",
        "Теперь же перейдем к обработке полученных слов. Можно использовать различные методы распознавания символов:\n",
        "\n",
        "1. Использование растровых шаблонов - подбор наиболее оптимального по выбранной метрике шаблона алфавита. Самый простой, однако наименее точный и наиболее трудозатратный метод [7]. С высокой точностью (около 96%) работает на печатных текстах и выявляет дефектные символы. Исполнен, к примеру, на Python в виде библиотеки 'ocr-Template-matching-'[8].\n",
        "2. Признаковые методы - основываются на вычислении матрицы признаков по каждому из символов и использовании поверх них обыкновенных классификаторов [4]. Таким образом, производится распознавание не символа, а набора его признаков, что сильно улучшает возможности работы с различными почерками и рукописным текстом в целом. Реализация: инструмент 'Azati OCR', библиотека Python 'cv2'.\n",
        "3. Структурные методы - используют представления о взаимном расположении символов и их сегментов, что уже гораздо точнее, чем шаблонная модель, применяется на символах с различным оформлением, но все еще чувствительно к нарушениям в начертании [3]. \n",
        "4. Структурно - пятенные методы - предполагают применения в качестве методов структурирования не только геометрические элементы, но и \"пятна\", то есть особые точки текста (изломы, пересечения и прочие) [Абраменко 2000]. Одни из самых успешно применяемых на практике. Используются, в том числе, инструментарием  'Tesseract OCR' и надстройкой Python 'pytesseract' [3].\n",
        "5. Нейросетевые методы - использующие различные технологии, в том числе архитектуры Resnet [6]. Плохо применимы для конвейерной обработки, однако достаточно точны. Легко собираются с помощью 'Keras' и 'TensorFlow' от Python.\n",
        "6. Модели Маркова  - используют вероятностные модели (основаны на построении цепей Маркова), предсказывают порядок слов в предложениях [2].\n",
        "\n",
        "Как можно было заметить, в последние годы число методов реализации всех этапов задач распознавания текстов серьезно увеличилось. Однако все они имеют разные способы применения, платформы запуска, инструменты сборки, точности и скорости работы. Далее мы продемонстрируем на собранной на практике тестовой выборке использование различных моделей: от эталонных до нейросетевых.\n",
        "\n",
        "Распознавание и математический анализ внешней информации компьютером в наше время процесс популярный, являющийся, в сущности, основой научной деятельности, бизнеса, систем безопасности и прочих немаловажных элементов повседневности. От относительно простых схем детектирования геометрических структур и цветов человечество быстро перешло к сложным фигурам, комбинациям объектов и, конечно же, текстовым данным.\n",
        "\n",
        "Компьютерный анализ рукописных документов востребован теперь в широком спектре задач, в том числе при организации деятельности крупных банков и страховых компаний [5]. В основе процесса лежит растровое изображение текста, переданное в память через периферийное устройство, ради упрощения анализа и редактирования требующее дальнейшей машинной обработки, методы которой различны и развиваются с каждым днем.\n",
        "\n",
        "В основе своей эти методы делятся на два кластера: «онлайн» и «оффлайн» [1]. «Оффлайн» подразумевает распознавание статичных данных, уже переданных полноценно в память компьютера (чем мы и займемся), а «онлайн» это, скорее, процесс слежения за динамической генерацией текста человеком в реальном мире ручкой или стилусом, что несколько проще.\n",
        "\n",
        "Поскольку не все люди имеют хороший почерк, и не все камеры производят чистые сэмплы, производство «оффлайн» обработки начинается с попытки максимально улучшить качество изображений (резкость, контрастность, четкость) путем обработки. Таким образом удается избавиться от некоторого процента искажений, перекосов, разрывов, клякс и прочих нюансах рукописного текста.\n",
        "\n",
        "Итак, начнем с предобработки фотографий. Прежде всего, конечно, происходит изменение размеров изображений к одному шаблону для корректной работы алгоритмов. Далее специалисты советуют начать со сглаживания спектра цветов изображения и шумоподавления, после чего происходит приведение палитры сэмпла к черно-белой. Это увеличит контрастность границ символов и уменьшит проблемы с сегментацией, выделит элементы на фоне. Для этого применим инструментарий библиотек Python 'PIL', и 'cv2' [1].\n",
        "\n",
        "Далее мы переходим к этапу сегментации структурных элементов обработанного документа: предложений и слов. \n",
        "\n",
        "И вновь мы, прежде всего, столкнемся с проблемой неточностей и геометрических ошибок (изгибов, асимметрии и проч.) даже на начальном этапе рассмотрения абзацев. Горизонтальные линии в тексте (на которые \"нанизаны\" наши предложения) выделяются также разными методами:\n",
        "\n",
        "1. Top-down метод - метод, основанный на проецировании изображений на горизонтальные прямые [4].\n",
        "2. Bottom-up метод - отдельные элементы группируются на основе их геометрических качеств [4].\n",
        "3. Разбиение на \"базовые линии\" - основанный на методах оптимизации алгоритм стягивания крупных сегментов документа к воображаемой прямой [3].\n",
        "\n",
        "Поверх выделенных предложений применяется, соответственно, структурирование отдельных слов, проблемы и искажения которых схожи с проблемами предложений. \n",
        "\n",
        "Процесс сегментации слов целиком проводится также схожими методами:\n",
        "\n",
        "1. Bottom-up метод - аналогично выделению строк происходит группировка компонент по геометрическим свойствам [2].\n",
        "2. Нейросетевой метод - предполагает анализ признаковой картины сегментов текста и выявление отдельных слов при помощи моделирования [3]. \n",
        "\n",
        "Теперь же перейдем к обработке полученных слов. Можно использовать различные методы распознавания символов:\n",
        "\n",
        "1. Использование растровых шаблонов - подбор наиболее оптимального по выбранной метрике шаблона алфавита. Самый простой, однако наименее точный и наиболее трудозатратный метод [7]. С высокой точностью (около 96%) работает на печатных текстах и выявляет дефектные символы. Исполнен, к примеру, на Python в виде библиотеки 'ocr-Template-matching-'[8].\n",
        "2. Признаковые методы - основываются на вычислении матрицы признаков по каждому из символов и использовании поверх них обыкновенных классификаторов [4]. Таким образом, производится распознавание не символа, а набора его признаков, что сильно улучшает возможности работы с различными почерками и рукописным текстом в целом. Реализация: инструмент 'Azati OCR', библиотека Python 'cv2'.\n",
        "3. Структурные методы - используют представления о взаимном расположении символов и их сегментов, что уже гораздо точнее, чем шаблонная модель, применяется на символах с различным оформлением, но все еще чувствительно к нарушениям в начертании [3]. \n",
        "4. Структурно - пятенные методы - предполагают применения в качестве методов структурирования не только геометрические элементы, но и \"пятна\", то есть особые точки текста (изломы, пересечения и прочие) [Абраменко 2000]. Одни из самых успешно применяемых на практике. Используются, в том числе, инструментарием  'Tesseract OCR' и надстройкой Python 'pytesseract' [3].\n",
        "5. Нейросетевые методы - использующие различные технологии, в том числе архитектуры Resnet [6]. Плохо применимы для конвейерной обработки, однако достаточно точны. Легко собираются с помощью 'Keras' и 'TensorFlow' от Python.\n",
        "6. Модели Маркова  - используют вероятностные модели (основаны на построении цепей Маркова), предсказывают порядок слов в предложениях [2].\n",
        "\n",
        "Как можно было заметить, в последние годы число методов реализации всех этапов задач распознавания текстов серьезно увеличилось. Однако все они имеют разные способы применения, платформы запуска, инструменты сборки, точности и скорости работы. Далее мы продемонстрируем на собранной на практике тестовой выборке использование различных моделей: от эталонных до нейросетевых."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3zNX2QTgOIJ"
      },
      "source": [
        "## Список используемой литературы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9dvieLlgT2W"
      },
      "source": [
        "1. Bunke, H and Bengio, Samy and Vinciarelli, A. Offline recognition of\n",
        "unconstrained handwritten texts using HMMs and statistical language\n",
        "models // Pattern Analysis and Machine Intelligence, IEEE Transactions\n",
        "on, 26, 6, (709–720), 2004.\n",
        "\n",
        "2. Plo ̈tz, Thomas and Fink, Gernot A. Markov models for offline\n",
        "handwriting recognition: a survey // International Journal on Document\n",
        "Analysis and Recognition (IJDAR), 12, 4 (269–298), 2009.\n",
        "\n",
        "3. Huang, Chen and Srihari, Sargur N. Word segmentation of off-line\n",
        "handwritten documents. Electronic Imaging 2008 (International Society\n",
        "for Optics and Photonics), (68150E–68150E), 2008.\n",
        "\n",
        "4. Гайдуков Н.П., Савкова Е.О.,\n",
        "Обзор методов распознавания рукописного текста.\n",
        "URL:\n",
        "http://masters.donntu.org/2012/fknt/gaydukov/library/5_gaydukov.pdf\n",
        "\n",
        "5. Распознавание рукописных текстов, Ижевск 2006, А. В. Кучуганов,\n",
        "Г. В. Лапинская.\n",
        "\n",
        "6. \"The First Census Optical Character Recognition System Conference\",\n",
        "R.A.Wilkinson et al., eds., . Tech. Report, NISTIR 4912, US Deop.\n",
        "Commerse, NIST, Gaithersburg, Md., 1992.\n",
        "\n",
        "7. Кучуганов и др. 2001 ― Кучуганов, В. Н. Система визуального\n",
        "проектирования баз знаний / В. Н. Кучуганов, И. Н. Габдрахманов //\n",
        "Информационные технологии в инновационных проектах: тр. III\n",
        "междунар. науч.-техн. конф. — Ижевск, 2001. ― С. 140–143.\n",
        "\n",
        "8. Кучуганов 2002 ― Кучуганов, В. Н. Семантика графической\n",
        "информации // Известия ТРТУ: Тематич. вып. «Интеллектуальные\n",
        "САПР» : материалы междунар. науч.-техн. конф. ― Таганрог: Изд-во\n",
        "ТРТУ, 2002. ― No 3 (26). ― С. 157–166. Кучуганов 2005 ―\n",
        "Кучуганов, В. Н. Визуальное моделирование текстов / В. Н.\n",
        "Кучуганов // «Интеллектуальные системы» (AIS'05) и\n",
        "«Интеллектуальные САПР» (CAD–2005): тр. междунар. науч.-\n",
        "технич. конф. — М.: ФИЗМАТЛИТ, 2005. — Т. 4. ― С. 104–114.\n",
        "Непейвода 1989 ― Непейвода, Н. Н. Некоторые семантические\n",
        "конструкции конструктивных логик схем программ / Н. Н. Непейвода\n",
        "// Вычислительные системы. ― Вып. 129. ― Новосибирск, 1989. ―\n",
        "С. 49–66."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qd5tvjAugc8k"
      },
      "source": [
        "# Протокол эксперимента"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iZOsNmQgh2f"
      },
      "source": [
        "## 1 Гипотезы и эксперименты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhX_IFnHgnPD"
      },
      "source": [
        "В процессе реализации архитектуры решения проверены следующие гипотезы:\n",
        "\n",
        "1. Использование альтернативного метода обучения и тестирования, построенного на собранных самостоятельно данных, может быть успешнее готовых методов зарубежных разработок.\n",
        "2. Использование инструментария LMDB даст выигрыш в скорости обучения.\n",
        "3. Тестирование и подбор инструментов коррекции орфографии в качестве дополнительного элемента декодера даст лучший результат."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De4WVb1LgoRu"
      },
      "source": [
        "## 2 Генерация выборки для обучения прототипов и тестирования уже имеющихся архитектур"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oCrzCK3vgste"
      },
      "source": [
        "Осмотрев уже имеющиеся в открытом доступе наборы данных для распознавания русского рукописного текста, мы пришли к выводу, что для обучения нашего прототипа и выигрыша у имеющихся инструментов OCR нам потребуется создать самостоятельно обучающую выборку. Для этого мы используем алгоритм (полу)автоматической аннотации текстов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFc-rta1gv26"
      },
      "source": [
        "### 2.1 **Собственное** ПО для полуавтоматической аннотации текстов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BDvq5Ivg1Pp"
      },
      "source": [
        "Алгоритм состоит из нескольких частей.\n",
        "Мы находим в открытом доступе несколько текстов русского языка: классическая литература, новостные статьи, служебная или научная литература для максимального разнообразия сочетания букв алфавита, слогов и подряд идущих слов. Затем мы реализуем минимальную обработку собранных текстов: удаление служебных символов, удаление чисел. Далее из имеющегося набора предложений (после обработки) мы создаем комбинации слов фиксированной длины, после чего создаем индивидуальные наборы текстов для разметки, перемешав имеющиеся тематические наборы.\n",
        "Затем мы реализуем разметку полученных сэмплов соответственным рукописным текстом. При этом мы используем для заполнения вручную данных людей разного пола, возраста и профессий, что может дать широкий спектр вариаций почерков, исключая переобучение моделей.\n",
        "\n",
        "Для оцифровки заполненных данных мы используем возможности библиотеки OpenCV (мы используем версию для Python) для детекции таблицы и распознавания ее ячеек. Для работы подходят как сканированные документы, так и снятые камерой мобильного телефона.\n",
        "\n",
        "После загрузки изображения производится его бинаризация и инвертирование.\n",
        "\n",
        "На следующем шаге мы определяем ядро для выделения прямоугольных рамок и в дальнейшем табличной структуры. Во-первых, мы определяем размер ядра, а также размеры горизонтального и вертикального ядер для детекции соответственно горизонтальных и вертикальных линий. Во-вторых, мы комбинируем полученные линии в новое изображение, присвоив обоим одинаковые веса (по 0.5). Такие манипуляции позволяют успешно извлечь табличную структуру изображения.\n",
        "\n",
        "После этого мы используем методы findCountours для определения контуров. Это позволяет нам выделить интересующие сегменты изображения (ячейки таблицы). Ячейки сортируются в определенном порядке, и в результате мы получаем изображения рукописного текста с заранее известными ответами (поскольку исходные изображения были получены распечаткой csv-файла и каждой строке соответствует словосочетание). После слияния текстовых данных и путей к файлам полученных изображений обучающий набор готов к загрузке в модель."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpwfLsrThToN"
      },
      "source": [
        "## 3 Тестирование готовых моделей и инструментов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lm4zNHPbhWym"
      },
      "source": [
        "### 3.1 Тестирование модели tesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5O_wcr6haSt"
      },
      "source": [
        "Используем оболочку pytesseract и собранные нами данные с различными аугментациями, оценим эффективность работы этого алгоритма"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZpHB02ThcoO"
      },
      "source": [
        "### 3.2 Тестирование шаблонной модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ_0oAkyhe7M"
      },
      "source": [
        "Аналогичные сценарии тестирования и метрики, используем разработанную в открытом доступе модель с собранными нами шаблонами символов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZBlQdEZhiQo"
      },
      "source": [
        "## 4 Обучение и тестирование собственной модели, оценка ее эффективности"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyuwMCBfhlA_"
      },
      "source": [
        "### 4.1 Архитектура модели"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wACAeCrehnqR"
      },
      "source": [
        "На этом этапе мы использовали архитектуру, основанную на базе LSTM: модель image-to-text, основанную на генерации feature map с помощью CNN, а затем распознавание текста символ за символом. \n",
        "\n",
        "Модель состоит из 5 слоев CNN, 2 слоев RNN (LSTM) и слоя Loss и декодирования."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxQmeQcehqDQ"
      },
      "source": [
        "#### 4.2 Метрики"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ns_jr-cGhtpZ"
      },
      "source": [
        "Character accuracy (CER) - метрика, показывающая посимвольное соответствие вывода модели и валидирующей строки (то есть доля верно предсказанных символов в целом).\n",
        "\n",
        "Line accuracy - метрика, показывающая соответствие вывода модели и валидирующей строки по словам (то есть доля верно предсказанных предложений)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHnErTdthuZ3"
      },
      "source": [
        "### 4.3 Результаты экспериментов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tzDwkW2h64i"
      },
      "source": [
        "Эксперимент 1.\n",
        "\n",
        "Модель: Tesseract (инструмент реализации: pytesseract)\n",
        "\n",
        "Время обучения: -\n",
        "\n",
        "Эпох: -\n",
        "\n",
        "Без перемешивания выборки\n",
        "\n",
        "Оптимизатор: -\n",
        "\n",
        "Декодер: -\n",
        "\n",
        "Размещение данных: Google Drive. \n",
        "\n",
        "CER: 57.4%\n",
        "\n",
        "Line accuracy: 2%\n",
        "\n",
        "Эксперимент 2.\n",
        "\n",
        "Модель: LSTM\n",
        "\n",
        "Время обучения: 2 ч 15 мин\n",
        "\n",
        "Эпох: 36\n",
        "\n",
        "Холодный старт\n",
        "\n",
        "Без перемешивания выборки\n",
        "\n",
        "Оптимизатор: Adam\n",
        "\n",
        "Декодер: CTC Bestpath\n",
        "\n",
        "Размещение данных: Google Drive. \n",
        "\n",
        "CER: 21%\n",
        "\n",
        "Line accuracy: 15%\n",
        "\n",
        "Эксперимент 3.\n",
        "\n",
        "Модель: LSTM\n",
        "\n",
        "Время обучения: 3 ч 35 мин\n",
        "\n",
        "Эпох: 166 (последние 25 без улучшений)\n",
        "\n",
        "Холодный старт\n",
        "\n",
        "Без перемешивания выборки\n",
        "\n",
        "Оптимизатор: Adam\n",
        "\n",
        "Декодер: CTC Bestpath\n",
        "\n",
        "Размещение данных: LMDB.\n",
        "\n",
        "CER: 11.0%\n",
        "\n",
        "Line accuracy: 26.73%\n",
        "\n",
        "Эксперимент 4.\n",
        "\n",
        "Модель: LSTM\n",
        "\n",
        "Время обучения: 3 ч 45 мин\n",
        "\n",
        "Эпох: 142 (последние 10 без улучшений)\n",
        "\n",
        "Холодный старт\n",
        "\n",
        "Перемешанная выборка с фиксированной псевдослучайностью\n",
        "\n",
        "Оптимизатор: Adam\n",
        "\n",
        "Декодер: CTC Beamsearch\n",
        "\n",
        "Размещение данных: LMDB.\n",
        "\n",
        "CER: 8.61%\n",
        "\n",
        "Line accuracy: 36.7%\n",
        "\n",
        "Эксперимент 5.\n",
        "\n",
        "Модель: LSTM\n",
        "\n",
        "Время обучения: 2 ч 30 мин\n",
        "\n",
        "Эпох: 88\n",
        "\n",
        "Холодный старт\n",
        "\n",
        "Перемешанная выборка с фиксированной псевдослучайностью\n",
        "\n",
        "Оптимизатор: Adam\n",
        "\n",
        "Декодер: CTC WordBeamsearch\n",
        "\n",
        "Размещение данных: LMDB\n",
        "\n",
        "CER: 4.17%\n",
        "\n",
        "Line accuracy: 75.52%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZOPbjHhh8XA"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSGxEIfOjm_O"
      },
      "source": [
        "### Выводы"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o7vh8OgFjouh"
      },
      "source": [
        "Эксперимент с инструментом pytesseract (эксперимент 1) показал серьезное отличие в худшую сторону относительно модели, обученной на собранных вручную данных (даже без оптимизации параметров и коррекции орфографии) и протестированной с идентичными условиями (эксперимент 2). Гипотеза 1 подтверждается.\n",
        "\n",
        "Внедрение технологии  LMDB хранения и подачи выборки данных (эксперимент 3) более, чем в 4 раза увеличивает скорость обучения модели при неизменных размерах и составе выборок, а также прочих параметрах задачи. Гипотеза 2 подтверждается.\n",
        "\n",
        "Использование альтернативной технологии декодирования Beamsearch (эксперимент 4), задействующей коррекцию орфографии распознаваемых слов, существенно увеличивает качество работы модели при относительно малом падении скорости ее обучения. Кроме того, самый высокий результат точности и самый и наибольшее время исполнения показал декодер WordBeamsearch (эксперимент 5), корректирующий ошибки на основе использования русскоязычного словаря (ограниченная версия, построенная на обучающей выборке). Гипотеза 3 подтверждается."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gQzeiMTjzR1"
      },
      "source": [
        "## 5 Гипотезы, требующие дальнейшей проверки "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aukt3z76kBGJ"
      },
      "source": [
        "1. Изменение архитектуры сети (больше-меньше свёрточных слоёв, больше LSTM, вставить LSTM в начало сети) может положительно влиять на качество обучения и работы модели. \n",
        "2. Наложение на работу сети оптимизатора гиперпараметров (от TF или байесовский [https://distill.pub/2020/bayesian-optimization/](https://distill.pub/2020/bayesian-optimization/)) даст положительный результат в контексте такой задачи.\n",
        "3. Добавление дополнительного датасета изображений (готовый размеченный, MNIST, сгенерированный специализированным инструментом) после приведения его к выбранному формату даст положительный результат.\n",
        "4. Прочие изменения оригинала изображений выборки: увеличение разрешение, добавление рамок белого цвета вокруг может положительно влиять на качество обучения и работы модели.\n",
        "5. Использование шаблонного метода должно быть наиболее качественным методом только в контексте распознавания слов только одного конкретного шрифта.\n",
        "6. Процесс обучения на комбинации слов (в альтернативу обучению на отдельных словах) даст лучший результат.\n",
        "7. Классификация авторов рукописных текстов по полу и возрасту возможна."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iDYfxj5kZJw"
      },
      "source": [
        "# Эксперименты"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIJc01rRk6pd"
      },
      "source": [
        "## Импорт библиотек и подготовка структур данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8Sd1vQOnle7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f74af4c3-844e-4b7b-e713-2fd93740e445"
      },
      "source": [
        "!touch corpus.txt\n",
        "!pip install path\n",
        "!pip install pyaspeller"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: path in /usr/local/lib/python3.7/dist-packages (16.2.0)\n",
            "Requirement already satisfied: pyaspeller in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pyaspeller) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pyaspeller) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pyaspeller) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pyaspeller) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pyaspeller) (2021.5.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOIMol062VX3"
      },
      "source": [
        "import random\n",
        "import csv\n",
        "import cv2\n",
        "import shutil\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import lmdb\n",
        "import pickle\n",
        "import editdistance\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from collections import namedtuple\n",
        "from typing import List, Tuple\n",
        "from path import Path\n",
        "from pyaspeller import YandexSpeller"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I0qxcPXHqb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6fd693-171f-4483-e184-eec371b9b747"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "drive_path = Path('/content/drive/MyDrive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3Qm81WvZ_wP"
      },
      "source": [
        "repo_path = Path(drive_path / 'CTCWordBeamSearch')\n",
        "if not Path('./CTCWordBeamSearch').exists():\n",
        "  shutil.copytree(repo_path, './CTCWordBeamSearch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOEBczUJQPJh",
        "collapsed": true,
        "outputId": "f96f5ddf-d80c-48d5-eb91-46d260e301df"
      },
      "source": [
        "!pip install /content/CTCWordBeamSearch --use-feature=in-tree-build"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing ./CTCWordBeamSearch\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from word-beam-search==1.0.1) (1.19.5)\n",
            "Building wheels for collected packages: word-beam-search\n",
            "  Building wheel for word-beam-search (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word-beam-search: filename=word_beam_search-1.0.1-cp37-cp37m-linux_x86_64.whl size=1178380 sha256=1ab72b8ab3c2c56f309671984454caed73e25cc701b66df38dd4b5f6ebf04099\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-u2n7qpbb/wheels/16/85/4b/44623da5b9e8f62d95f1dda76b4944e98821e531c220d5058c\n",
            "Successfully built word-beam-search\n",
            "Installing collected packages: word-beam-search\n",
            "  Attempting uninstall: word-beam-search\n",
            "    Found existing installation: word-beam-search 1.0.1\n",
            "    Uninstalling word-beam-search-1.0.1:\n",
            "      Successfully uninstalled word-beam-search-1.0.1\n",
            "Successfully installed word-beam-search-1.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdnHYn-kks_y"
      },
      "source": [
        "Две основные сущности - семпл и батч - представлены именованными кортежами. Именно их генерирует загрузчик"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8sUT2pw_HxJ"
      },
      "source": [
        "Sample = namedtuple('Sample', 'gt_text, file_path')\n",
        "Batch = namedtuple('Batch', 'imgs, gt_texts, batch_size')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubZ2hxN8YXdN"
      },
      "source": [
        "## БД lmdb для быстрой загрузки изображений"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcWt3oA8bOX"
      },
      "source": [
        "### Создание БД"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q8v6LvGdYWUI"
      },
      "source": [
        "# БД на 1 гиг\n",
        "# env = lmdb.open('lmdb', map_size=1024 * 1024 * 1024)\n",
        "# imgs = (drive_path / 'images').walkfiles('*.jpg')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "L53sbotuZubg"
      },
      "source": [
        "# with env.begin(write=True) as conn:\n",
        "#     for idx, img in enumerate(imgs):\n",
        "#         if idx % 400 == 0:\n",
        "#           print(idx)\n",
        "#         read_img = cv2.imread(img, cv2.IMREAD_GRAYSCALE)\n",
        "#         bd_filename = '/'.join(str(img).split('/')[-3:]).encode(\"ascii\")\n",
        "#         conn.put(bd_filename, pickle.dumps(read_img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fQPVJxWTbp8T"
      },
      "source": [
        "# env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx2jHz3H8iQh"
      },
      "source": [
        "### Загрузка готовой БД"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "K5DIMMKU8hkO"
      },
      "source": [
        "lmdb_path = Path(drive_path / 'lmdb')\n",
        "if not Path('./lmdb').exists():\n",
        "  shutil.copytree(lmdb_path, 'lmdb')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKyXDYFFkgvv"
      },
      "source": [
        "## Предобработчик"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRN51pNnkfQ9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a2034888-1011-44f9-fce5-bff421cc1edc"
      },
      "source": [
        "class Preprocessor:\n",
        "    def __init__(self,\n",
        "                 img_size: Tuple[int, int],\n",
        "                 padding: int = 0,\n",
        "                 dynamic_width: bool = False,\n",
        "                 data_augmentation: bool = False,\n",
        "                 line_mode: bool = False) -> None:\n",
        "        # dynamic width only supported when no data augmentation happens\n",
        "        assert not (dynamic_width and data_augmentation)\n",
        "        # when padding is on, we need dynamic width enabled\n",
        "        assert not (padding > 0 and not dynamic_width)\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.padding = padding\n",
        "        self.dynamic_width = dynamic_width\n",
        "        self.data_augmentation = data_augmentation\n",
        "        self.line_mode = line_mode\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def _truncate_label(text: str, max_text_len: int) -> str:\n",
        "        \"\"\"\n",
        "        Function ctc_loss can't compute loss if it cannot find a mapping between text label and input\n",
        "        labels. Repeat letters cost double because of the blank symbol needing to be inserted.\n",
        "        If a too-long label is provided, ctc_loss returns an infinite gradient.\n",
        "        \"\"\"\n",
        "        cost = 0\n",
        "        for i in range(len(text)):\n",
        "            if i != 0 and text[i] == text[i - 1]:\n",
        "                cost += 2\n",
        "            else:\n",
        "                cost += 1\n",
        "            if cost > max_text_len:\n",
        "                return text[:i]\n",
        "        return text\n",
        "\n",
        "    \n",
        "    def process_img(self, img: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Resize to target size, apply data augmentation.\"\"\"\n",
        "\n",
        "        # data augmentation\n",
        "        img = img.astype(np.float)\n",
        "        if self.data_augmentation:\n",
        "            # photometric data augmentation\n",
        "            if random.random() < 0.25:\n",
        "                def rand_odd():\n",
        "                    return random.randint(1, 3) * 2 + 1\n",
        "                img = cv2.GaussianBlur(img, (rand_odd(), rand_odd()), 0)\n",
        "            if random.random() < 0.25:\n",
        "                img = cv2.dilate(img, np.ones((3, 3)))\n",
        "            if random.random() < 0.25:\n",
        "                img = cv2.erode(img, np.ones((3, 3)))\n",
        "\n",
        "            # geometric data augmentation\n",
        "            wt, ht = self.img_size\n",
        "            h, w = img.shape\n",
        "            f = min(wt / w, ht / h)\n",
        "            fx = f * np.random.uniform(0.75, 1.05)\n",
        "            fy = f * np.random.uniform(0.75, 1.05)\n",
        "\n",
        "            # random position around center\n",
        "            txc = (wt - w * fx) / 2\n",
        "            tyc = (ht - h * fy) / 2\n",
        "            freedom_x = max((wt - fx * w) / 2, 0)\n",
        "            freedom_y = max((ht - fy * h) / 2, 0)\n",
        "            tx = txc + np.random.uniform(-freedom_x, freedom_x)\n",
        "            ty = tyc + np.random.uniform(-freedom_y, freedom_y)\n",
        "\n",
        "            # map image into target image\n",
        "            M = np.float32([[fx, 0, tx], [0, fy, ty]])\n",
        "            target = np.ones(self.img_size[::-1]) * 255\n",
        "            img = cv2.warpAffine(img, M, dsize=self.img_size, dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
        "\n",
        "            # photometric data augmentation\n",
        "            if random.random() < 0.5:\n",
        "                img = img * (0.25 + random.random() * 0.75)\n",
        "            if random.random() < 0.25:\n",
        "                img = np.clip(img + (np.random.random(img.shape) - 0.5) * random.randint(1, 25), 0, 255)\n",
        "            if random.random() < 0.1:\n",
        "                img = 255 - img\n",
        "\n",
        "        # no data augmentation\n",
        "        else:\n",
        "            if self.dynamic_width:\n",
        "                ht = self.img_size[1]\n",
        "                h, w = img.shape\n",
        "                f = ht / h\n",
        "                wt = int(f * w + self.padding)\n",
        "                wt = wt + (4 - wt) % 4\n",
        "                tx = (wt - w * f) / 2\n",
        "                ty = 0\n",
        "            else:\n",
        "                wt, ht = self.img_size\n",
        "                h, w = img.shape\n",
        "                f = min(wt / w, ht / h)\n",
        "                tx = (wt - w * f) / 2\n",
        "                ty = (ht - h * f) / 2\n",
        "\n",
        "            # map image into target image\n",
        "            M = np.float32([[f, 0, tx], [0, f, ty]])\n",
        "            target = np.ones([ht, wt]) * 255\n",
        "            img = cv2.warpAffine(img, M, dsize=(wt, ht), dst=target, borderMode=cv2.BORDER_TRANSPARENT)\n",
        "\n",
        "        # transpose for TF\n",
        "        img = cv2.transpose(img)\n",
        "\n",
        "        # convert to range [-1, 1]\n",
        "        img = img / 255 - 0.5\n",
        "        return img\n",
        "\n",
        "    \n",
        "    def process_batch(self, batch: Batch) -> Batch:\n",
        "\n",
        "        res_imgs = [self.process_img(img) for img in batch.imgs]\n",
        "        max_text_len = res_imgs[0].shape[0] // 4\n",
        "        res_gt_texts = [self._truncate_label(gt_text, max_text_len) for gt_text in batch.gt_texts]\n",
        "        return Batch(res_imgs, res_gt_texts, batch.batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-db053da10d33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mPreprocessor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     def __init__(self,\n\u001b[1;32m      3\u001b[0m                  \u001b[0mimg_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                  \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mdynamic_width\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-db053da10d33>\u001b[0m in \u001b[0;36mPreprocessor\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                  \u001b[0mdynamic_width\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                  \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                  line_mode: bool = False) -> None:\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0;31m# dynamic width only supported when no data augmentation happens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdynamic_width\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdata_augmentation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'Tuple' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jXhNYVdkaDl"
      },
      "source": [
        "## Загрузчик"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPmPVvbWXqNY"
      },
      "source": [
        "class DataLoader:\n",
        "  def __init__(self,\n",
        "                data_dir: Path,\n",
        "                batch_size: int,\n",
        "                data_split: float = 0.9,\n",
        "                fast: bool = False) -> None:\n",
        "        \"\"\"Загрузчик датасета\"\"\"\n",
        "\n",
        "        assert data_dir.exists()\n",
        "\n",
        "        self.fast = fast\n",
        "        if fast:\n",
        "            self.env = lmdb.open('lmdb', readonly=True)\n",
        "\n",
        "        self.data_augmentation = False\n",
        "        self.curr_idx = 0  # порядковый номер в выборке\n",
        "        self.batch_size = batch_size\n",
        "        self.samples = []\n",
        "\n",
        "        labels_path = Path.joinpath(data_dir, 'labels')\n",
        "        file = open(labels_path / 'HTR.csv')\n",
        "        reader = csv.reader(file)\n",
        "        next(reader)  # пропуск заголовка\n",
        "\n",
        "        for row in reader:\n",
        "          gt_text, img_name = row\n",
        "          img_path = Path.joinpath(data_dir, img_name[2:])  # нужно для colab, т.к. пути начинаются с ./ \n",
        "          self.samples.append(Sample(gt_text, img_path))\n",
        "          \n",
        "        file.close()\n",
        "\n",
        "        # фиксированное деление выборки (по умолчанию на валидацию 10%)\n",
        "        split_idx = int(data_split * len(self.samples))\n",
        "        random.seed(6)\n",
        "        random.shuffle(self.samples)\n",
        "        self.train_samples = self.samples[:split_idx]\n",
        "        self.validation_samples = self.samples[split_idx:]\n",
        "\n",
        "        self.train_texts = [x.gt_text for x in self.train_samples]\n",
        "        self.validation_texts = [x.gt_text for x in self.validation_samples]\n",
        "\n",
        "        # start with train set\n",
        "        self.train_set()\n",
        "    \n",
        "\n",
        "  def train_set(self) -> None:\n",
        "        \"\"\"Случайно выбранная подвыборка из тренировочного набора данных\"\"\"\n",
        "        self.data_augmentation = True\n",
        "        self.curr_idx = 0\n",
        "        random.seed()\n",
        "        random.shuffle(self.train_samples)\n",
        "        self.samples = self.train_samples\n",
        "        self.curr_set = 'train'\n",
        "\n",
        "    \n",
        "  def validation_set(self) -> None:\n",
        "        \"\"\"Вылидационный сет\"\"\"\n",
        "        self.data_augmentation = False\n",
        "        self.curr_idx = 0\n",
        "        self.samples = self.validation_samples\n",
        "        self.curr_set = 'val'\n",
        "\n",
        "\n",
        "  def get_iterator_info(self) -> Tuple[int, int]:\n",
        "        \"\"\"Возвращает номер текущего батча и общее число батчей\"\"\"\n",
        "        if self.curr_set == 'train':\n",
        "            num_batches = int(np.floor(len(self.samples) / self.batch_size))  # в тренирочной выборке должны быть только полноразмерные батчи\n",
        "        else:\n",
        "            num_batches = int(np.ceil(len(self.samples) / self.batch_size))  # в валидационной последний батч может быть меньше\n",
        "        curr_batch = self.curr_idx // self.batch_size + 1\n",
        "        return curr_batch, num_batches\n",
        "\n",
        "\n",
        "  def has_next(self) -> bool:\n",
        "        \"\"\"Возвращает флаг, показывающий, хватает ли данных на еще один батч\"\"\"\n",
        "        if self.curr_set == 'train':\n",
        "            return self.curr_idx + self.batch_size <= len(self.samples)  # в тренирочной выборке должны быть только полноразмерные батчи\n",
        "        else:\n",
        "            return self.curr_idx < len(self.samples)  # в валидационной последний батч может быть меньше\n",
        "\n",
        "\n",
        "  def _get_img(self, i: int) -> np.ndarray:\n",
        "        if self.fast:\n",
        "            with self.env.begin() as conn:\n",
        "                bd_filename = '-'.join(self.samples[i].file_path.split('/')[-3:]).encode(\"ascii\")\n",
        "                data = conn.get(bd_filename)\n",
        "                img = pickle.loads(data)\n",
        "        else:\n",
        "            img = cv2.imread(self.samples[i].file_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "        return img\n",
        "\n",
        "\n",
        "  def get_batch(self) -> Batch:\n",
        "        \"\"\"Получить батч\"\"\"\n",
        "        batch_range = range(self.curr_idx, min(self.curr_idx + self.batch_size, len(self.samples)))\n",
        "\n",
        "        imgs = [self._get_img(i) for i in batch_range]\n",
        "        gt_texts = [self.samples[i].gt_text for i in batch_range]\n",
        "\n",
        "        self.curr_idx += self.batch_size\n",
        "        return Batch(imgs, gt_texts, len(imgs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_o23Fs6s1Cos"
      },
      "source": [
        "## Модель"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GV8B6ah1Eyj"
      },
      "source": [
        "# Disable eager mode\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "class DecoderType:\n",
        "    \"\"\"CTC decoder types.\"\"\"\n",
        "    BestPath = 0\n",
        "    BeamSearch = 1\n",
        "    WordBeamSearch = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYyH7HNuRsH2"
      },
      "source": [
        "class Model:\n",
        "    \"\"\"Minimalistic TF model for HTR.\"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 char_list: List[str],\n",
        "                 decoder_type: str = DecoderType.BestPath,\n",
        "                 must_restore: bool = False,\n",
        "                 dump: bool = False) -> None:\n",
        "        \"\"\"Init model: add CNN, RNN and CTC and initialize TF.\"\"\"\n",
        "        self.dump = dump\n",
        "        self.char_list = char_list\n",
        "        self.decoder_type = decoder_type\n",
        "        self.must_restore = must_restore\n",
        "        self.snap_ID = 0\n",
        "\n",
        "        # Whether to use normalization over a batch or a population\n",
        "        self.is_train = tf.compat.v1.placeholder(tf.bool, name='is_train')\n",
        "\n",
        "        # input image batch\n",
        "        self.input_imgs = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None))\n",
        "\n",
        "        # setup CNN, RNN and CTC\n",
        "        self.setup_cnn()\n",
        "        self.setup_rnn()\n",
        "        self.setup_ctc()\n",
        "\n",
        "        # setup optimizer to train NN\n",
        "        self.batches_trained = 0\n",
        "        self.update_ops = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
        "        with tf.control_dependencies(self.update_ops):\n",
        "            self.optimizer = tf.compat.v1.train.AdamOptimizer().minimize(self.loss)\n",
        "\n",
        "        # initialize TF\n",
        "        self.sess, self.saver = self.setup_tf()\n",
        "\n",
        "    def setup_cnn(self) -> None:\n",
        "        \"\"\"Create CNN layers.\"\"\"\n",
        "        cnn_in4d = tf.expand_dims(input=self.input_imgs, axis=3)\n",
        "\n",
        "        # list of parameters for the layers\n",
        "        kernel_vals = [5, 5, 3, 3, 3]\n",
        "        feature_vals = [1, 32, 64, 128, 128, 256]\n",
        "        stride_vals = pool_vals = [(2, 2), (2, 2), (1, 2), (1, 2), (1, 2)]\n",
        "        num_layers = len(stride_vals)\n",
        "\n",
        "        # create layers\n",
        "        pool = cnn_in4d  # input to first CNN layer\n",
        "        for i in range(num_layers):\n",
        "            kernel = tf.Variable(\n",
        "                tf.random.truncated_normal([kernel_vals[i], kernel_vals[i], feature_vals[i], feature_vals[i + 1]],\n",
        "                                           stddev=0.1))\n",
        "            conv = tf.nn.conv2d(input=pool, filters=kernel, padding='SAME', strides=(1, 1, 1, 1))\n",
        "            conv_norm = tf.compat.v1.layers.batch_normalization(conv, training=self.is_train)\n",
        "            relu = tf.nn.relu(conv_norm)\n",
        "            pool = tf.nn.max_pool2d(input=relu, ksize=(1, pool_vals[i][0], pool_vals[i][1], 1),\n",
        "                                    strides=(1, stride_vals[i][0], stride_vals[i][1], 1), padding='VALID')\n",
        "\n",
        "        self.cnn_out_4d = pool\n",
        "\n",
        "    def setup_rnn(self) -> None:\n",
        "        \"\"\"Create RNN layers.\"\"\"\n",
        "        rnn_in3d = tf.squeeze(self.cnn_out_4d, axis=[2])\n",
        "\n",
        "        # basic cells which is used to build RNN\n",
        "        num_hidden = 256\n",
        "        cells = [tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=num_hidden, state_is_tuple=True) for _ in\n",
        "                 range(2)]  # 2 layers\n",
        "\n",
        "        # stack basic cells\n",
        "        stacked = tf.compat.v1.nn.rnn_cell.MultiRNNCell(cells, state_is_tuple=True)\n",
        "\n",
        "        # bidirectional RNN\n",
        "        # BxTxF -> BxTx2H\n",
        "        (fw, bw), _ = tf.compat.v1.nn.bidirectional_dynamic_rnn(cell_fw=stacked, cell_bw=stacked, inputs=rnn_in3d,\n",
        "                                                                dtype=rnn_in3d.dtype)\n",
        "\n",
        "        # BxTxH + BxTxH -> BxTx2H -> BxTx1X2H\n",
        "        concat = tf.expand_dims(tf.concat([fw, bw], 2), 2)\n",
        "\n",
        "        # project output to chars (including blank): BxTx1x2H -> BxTx1xC -> BxTxC\n",
        "        kernel = tf.Variable(tf.random.truncated_normal([1, 1, num_hidden * 2, len(self.char_list) + 1], stddev=0.1))\n",
        "        self.rnn_out_3d = tf.squeeze(tf.nn.atrous_conv2d(value=concat, filters=kernel, rate=1, padding='SAME'),\n",
        "                                     axis=[2])\n",
        "\n",
        "    def setup_ctc(self) -> None:\n",
        "        \"\"\"Create CTC loss and decoder.\"\"\"\n",
        "        # BxTxC -> TxBxC\n",
        "        self.ctc_in_3d_tbc = tf.transpose(a=self.rnn_out_3d, perm=[1, 0, 2])\n",
        "        # ground truth text as sparse tensor\n",
        "        self.gt_texts = tf.SparseTensor(tf.compat.v1.placeholder(tf.int64, shape=[None, 2]),\n",
        "                                        tf.compat.v1.placeholder(tf.int32, [None]),\n",
        "                                        tf.compat.v1.placeholder(tf.int64, [2]))\n",
        "\n",
        "        # calc loss for batch\n",
        "        self.seq_len = tf.compat.v1.placeholder(tf.int32, [None])\n",
        "        self.loss = tf.reduce_mean(\n",
        "            input_tensor=tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.ctc_in_3d_tbc,\n",
        "                                                  sequence_length=self.seq_len,\n",
        "                                                  ctc_merge_repeated=True))\n",
        "\n",
        "        # calc loss for each element to compute label probability\n",
        "        self.saved_ctc_input = tf.compat.v1.placeholder(tf.float32,\n",
        "                                                        shape=[None, None, len(self.char_list) + 1])\n",
        "        self.loss_per_element = tf.compat.v1.nn.ctc_loss(labels=self.gt_texts, inputs=self.saved_ctc_input,\n",
        "                                                         sequence_length=self.seq_len, ctc_merge_repeated=True)\n",
        "\n",
        "        # best path decoding or beam search decoding\n",
        "        if self.decoder_type == DecoderType.BestPath:\n",
        "            self.decoder = tf.nn.ctc_greedy_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len)\n",
        "        elif self.decoder_type == DecoderType.BeamSearch:\n",
        "            self.decoder = tf.nn.ctc_beam_search_decoder(inputs=self.ctc_in_3d_tbc, sequence_length=self.seq_len,\n",
        "                                                         beam_width=50)\n",
        "        # word beam search decoding \n",
        "        elif self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            # prepare information about language (dictionary, characters in dataset, characters forming words)\n",
        "            chars = ''.join(self.char_list)\n",
        "            word_chars = ''.join(self.char_list[:-1])\n",
        "            corpus = open('corpus.txt').read()\n",
        "            # decode using the \"Words\" mode of word beam search\n",
        "            from word_beam_search import WordBeamSearch\n",
        "            self.decoder = WordBeamSearch(25, 'Words', 0.0, corpus.encode('utf8'), chars.encode('utf8'),\n",
        "                                          word_chars.encode('utf8'))\n",
        "\n",
        "            # the input to the decoder must have softmax already applied\n",
        "            self.wbs_input = tf.nn.softmax(self.ctc_in_3d_tbc, axis=2)\n",
        "\n",
        "    def setup_tf(self) -> Tuple[tf.compat.v1.Session, tf.compat.v1.train.Saver]:\n",
        "        \"\"\"Initialize TF.\"\"\"\n",
        "        print('Python: ' + sys.version)\n",
        "        print('Tensorflow: ' + tf.__version__)\n",
        "        sess = tf.compat.v1.Session()  # TF session\n",
        "\n",
        "        saver = tf.compat.v1.train.Saver(max_to_keep=1)  # saver saves model to file\n",
        "        model_dir = './model/'\n",
        "        latest_snapshot = tf.train.latest_checkpoint(model_dir)  # is there a saved model?\n",
        "\n",
        "        # if model must be restored (for inference), there must be a snapshot\n",
        "        if self.must_restore and not latest_snapshot:\n",
        "            raise Exception('No saved model found in: ' + model_dir)\n",
        "\n",
        "        # load saved model if available\n",
        "        if latest_snapshot:\n",
        "            print('Init with stored values from ' + latest_snapshot)\n",
        "            saver.restore(sess, latest_snapshot)\n",
        "        else:\n",
        "            print('Init with new values')\n",
        "            sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "        return sess, saver\n",
        "\n",
        "    def to_sparse(self, texts: List[str]) -> Tuple[List[List[int]], List[int], List[int]]:\n",
        "        \"\"\"Put ground truth texts into sparse tensor for ctc_loss.\"\"\"\n",
        "        indices = []\n",
        "        values = []\n",
        "        shape = [len(texts), 0]  # last entry must be max(labelList[i])\n",
        "\n",
        "        # go over all texts\n",
        "        for batchElement, text in enumerate(texts):\n",
        "            # convert to string of label (i.e. class-ids)\n",
        "            label_str = [self.char_list.index(c) for c in text]\n",
        "            # sparse tensor must have size of max. label-string\n",
        "            if len(label_str) > shape[1]:\n",
        "                shape[1] = len(label_str)\n",
        "            # put each label into sparse tensor\n",
        "            for i, label in enumerate(label_str):\n",
        "                indices.append([batchElement, i])\n",
        "                values.append(label)\n",
        "\n",
        "        return indices, values, shape\n",
        "\n",
        "    def decoder_output_to_text(self, ctc_output: tuple, batch_size: int) -> List[str]:\n",
        "        \"\"\"Extract texts from output of CTC decoder.\"\"\"\n",
        "\n",
        "        # word beam search: already contains label strings\n",
        "        if self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            label_strs = ctc_output\n",
        "\n",
        "        # TF decoders: label strings are contained in sparse tensor\n",
        "        else:\n",
        "            # ctc returns tuple, first element is SparseTensor\n",
        "            decoded = ctc_output[0][0]\n",
        "\n",
        "            # contains string of labels for each batch element\n",
        "            label_strs = [[] for _ in range(batch_size)]\n",
        "\n",
        "            # go over all indices and save mapping: batch -> values\n",
        "            for (idx, idx2d) in enumerate(decoded.indices):\n",
        "                label = decoded.values[idx]\n",
        "                batch_element = idx2d[0]  # index according to [b,t]\n",
        "                label_strs[batch_element].append(label)\n",
        "\n",
        "        # map labels to chars for all batch elements\n",
        "        # return [''.join([self.char_list[c] for c in labelStr]) for labelStr in label_strs]\n",
        "        output_dec = [''.join([self.char_list[c] for c in labelStr]) for labelStr in label_strs]\n",
        "        final_output = []\n",
        "\n",
        "        for item in output_dec:\n",
        "          try:\n",
        "            item_spelled = speller.spelled(item)\n",
        "            final_output.append(item_spelled)\n",
        "          except:\n",
        "            final_output.append(item)\n",
        "\n",
        "        return final_output\n",
        "\n",
        "    def train_batch(self, batch: Batch) -> float:\n",
        "        \"\"\"Feed a batch into the NN to train it.\"\"\"\n",
        "        num_batch_elements = len(batch.imgs)\n",
        "        max_text_len = batch.imgs[0].shape[0] // 4\n",
        "        sparse = self.to_sparse(batch.gt_texts)\n",
        "        eval_list = [self.optimizer, self.loss]\n",
        "        feed_dict = {self.input_imgs: batch.imgs, self.gt_texts: sparse,\n",
        "                     self.seq_len: [max_text_len] * num_batch_elements, self.is_train: True}\n",
        "        _, loss_val = self.sess.run(eval_list, feed_dict)\n",
        "        self.batches_trained += 1\n",
        "        return loss_val\n",
        "\n",
        "    @staticmethod\n",
        "    def dump_nn_output(rnn_output: np.ndarray) -> None:\n",
        "        \"\"\"Dump the output of the NN to CSV file(s).\"\"\"\n",
        "        dump_dir = './dump/'\n",
        "        if not os.path.isdir(dump_dir):\n",
        "            os.mkdir(dump_dir)\n",
        "\n",
        "        # iterate over all batch elements and create a CSV file for each one\n",
        "        max_t, max_b, max_c = rnn_output.shape\n",
        "        for b in range(max_b):\n",
        "            csv = ''\n",
        "            for t in range(max_t):\n",
        "                for c in range(max_c):\n",
        "                    csv += str(rnn_output[t, b, c]) + ';'\n",
        "                csv += '\\n'\n",
        "            fn = dump_dir + 'rnnOutput_' + str(b) + '.csv'\n",
        "            print('Write dump of NN to file: ' + fn)\n",
        "            with open(fn, 'w') as f:\n",
        "                f.write(csv)\n",
        "\n",
        "    def infer_batch(self, batch: Batch, calc_probability: bool = False, probability_of_gt: bool = False):\n",
        "        \"\"\"Feed a batch into the NN to recognize the texts.\"\"\"\n",
        "\n",
        "        # decode, optionally save RNN output\n",
        "        num_batch_elements = len(batch.imgs)\n",
        "\n",
        "        # put tensors to be evaluated into list\n",
        "        eval_list = []\n",
        "\n",
        "        if self.decoder_type == DecoderType.WordBeamSearch:\n",
        "            eval_list.append(self.wbs_input)\n",
        "        else:\n",
        "            eval_list.append(self.decoder)\n",
        "\n",
        "        if self.dump or calc_probability:\n",
        "            eval_list.append(self.ctc_in_3d_tbc)\n",
        "\n",
        "        # sequence length depends on input image size (model downsizes width by 4)\n",
        "        max_text_len = batch.imgs[0].shape[0] // 4\n",
        "\n",
        "        # dict containing all tensor fed into the model\n",
        "        feed_dict = {self.input_imgs: batch.imgs, self.seq_len: [max_text_len] * num_batch_elements,\n",
        "                     self.is_train: False}\n",
        "\n",
        "        # evaluate model\n",
        "        eval_res = self.sess.run(eval_list, feed_dict)\n",
        "        # TF decoders: decoding already done in TF graph\n",
        "        if self.decoder_type != DecoderType.WordBeamSearch:\n",
        "            decoded = eval_res[0]\n",
        "        # word beam search decoder: decoding is done in C++ function compute()\n",
        "        else:\n",
        "            decoded = self.decoder.compute(eval_res[0])\n",
        "\n",
        "        # map labels (numbers) to character string\n",
        "        texts = self.decoder_output_to_text(decoded, num_batch_elements)\n",
        "\n",
        "        # feed RNN output and recognized text into CTC loss to compute labeling probability\n",
        "        probs = None\n",
        "        if calc_probability:\n",
        "            sparse = self.to_sparse(batch.gt_texts) if probability_of_gt else self.to_sparse(texts)\n",
        "            ctc_input = eval_res[1]\n",
        "            eval_list = self.loss_per_element\n",
        "            feed_dict = {self.saved_ctc_input: ctc_input, self.gt_texts: sparse,\n",
        "                         self.seq_len: [max_text_len] * num_batch_elements, self.is_train: False}\n",
        "            loss_vals = self.sess.run(eval_list, feed_dict)\n",
        "            probs = np.exp(-loss_vals)\n",
        "\n",
        "        # dump the output of the NN to CSV file(s)\n",
        "        if self.dump:\n",
        "            self.dump_nn_output(eval_res[1])\n",
        "\n",
        "        return texts, probs\n",
        "\n",
        "    def save(self) -> None:\n",
        "        \"\"\"Save model to file.\"\"\"\n",
        "        self.snap_ID += 1\n",
        "        self.saver.save(self.sess, './model/snapshot', global_step=self.snap_ID)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbmaUYu9kmsh"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpHQglO7Zxkm"
      },
      "source": [
        "### Подготовка"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIhHIj9DYK96"
      },
      "source": [
        "class FilePaths:\n",
        "    \"\"\"Filenames and paths to data.\"\"\"\n",
        "    fn_summary = 'summary.json'\n",
        "    fn_corpus = 'corpus.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux1Bia1YZkwo"
      },
      "source": [
        "def write_summary(char_error_rates: List[float], word_accuracies: List[float]) -> None:\n",
        "    \"\"\"Writes training summary file for NN.\"\"\"\n",
        "    with open(FilePaths.fn_summary, 'w') as f:\n",
        "        json.dump({'charErrorRates': char_error_rates, 'wordAccuracies': word_accuracies}, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34CBYRkEfapI"
      },
      "source": [
        "**Здесь задаем целевой размер изображения**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6wJkPNMfVB5"
      },
      "source": [
        "img_size = (330, 40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DxL0s0jZaZUp"
      },
      "source": [
        "#### train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_L36E8kraQjk"
      },
      "source": [
        "def train(model: Model,\n",
        "          loader: DataLoader,\n",
        "          early_stopping: int = 25) -> None:\n",
        "    \"\"\"Trains NN.\"\"\"\n",
        "    epoch = 0  # число эпох с момента начала обучения\n",
        "    summary_char_error_rates = []\n",
        "    summary_word_accuracies = []\n",
        "    preprocessor = Preprocessor(img_size, data_augmentation=True)\n",
        "    best_char_error_rate = float('inf')  # лучший показатель символьной ошибки на валидации\n",
        "    no_improvement_since = 0  # число эпох без улучшения символьной ошибки\n",
        "\n",
        "    while True:\n",
        "        epoch += 1\n",
        "        print('Epoch:', epoch)\n",
        "\n",
        "        # обучение\n",
        "        print('Train NN')\n",
        "        loader.train_set()\n",
        "        while loader.has_next():\n",
        "            iter_info = loader.get_iterator_info()\n",
        "            batch = loader.get_batch()\n",
        "            batch = preprocessor.process_batch(batch)\n",
        "            loss = model.train_batch(batch)\n",
        "            print(f'Epoch: {epoch} Batch: {iter_info[0]}/{iter_info[1]} Loss: {loss}')\n",
        "\n",
        "        # валидация\n",
        "        char_error_rate, word_accuracy = validate(model, loader)\n",
        "\n",
        "        # запись статистики\n",
        "        summary_char_error_rates.append(char_error_rate)\n",
        "        summary_word_accuracies.append(word_accuracy)\n",
        "        write_summary(summary_char_error_rates, summary_word_accuracies)\n",
        "\n",
        "        # сохраняем модель, если качество на валидации улучшилось\n",
        "        if char_error_rate < best_char_error_rate:\n",
        "            print('Character error rate improved, save model')\n",
        "            best_char_error_rate = char_error_rate\n",
        "            no_improvement_since = 0\n",
        "            model.save()\n",
        "        else:\n",
        "            print(f'Character error rate not improved, best so far: {best_char_error_rate * 100.0}%')\n",
        "            no_improvement_since += 1\n",
        "\n",
        "        # остановка обучения, если качество не улучшалось последние N эпох\n",
        "        if no_improvement_since >= early_stopping:\n",
        "            print(f'No more improvement since {early_stopping} epochs. Training stopped.')\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC0FhPYGadd5"
      },
      "source": [
        "#### validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6534W9qaGkX"
      },
      "source": [
        "def validate(model: Model, loader: DataLoader) -> Tuple[float, float]:\n",
        "    \"\"\"Validates NN.\"\"\"\n",
        "    print('Validate NN')\n",
        "    loader.validation_set()\n",
        "    preprocessor = Preprocessor(img_size)\n",
        "    num_char_err = 0\n",
        "    num_char_total = 0\n",
        "    num_word_ok = 0\n",
        "    num_word_total = 0\n",
        "    while loader.has_next():\n",
        "        iter_info = loader.get_iterator_info()\n",
        "        print(f'Batch: {iter_info[0]} / {iter_info[1]}')\n",
        "        batch = loader.get_batch()\n",
        "        batch = preprocessor.process_batch(batch)\n",
        "        recognized, _ = model.infer_batch(batch)\n",
        "\n",
        "        print('Ground truth -> Recognized')\n",
        "        for i in range(len(recognized)):\n",
        "            num_word_ok += 1 if batch.gt_texts[i] == recognized[i] else 0\n",
        "            num_word_total += 1\n",
        "            dist = editdistance.eval(recognized[i], batch.gt_texts[i])\n",
        "            num_char_err += dist\n",
        "            num_char_total += len(batch.gt_texts[i])\n",
        "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gt_texts[i] + '\"', '->',\n",
        "                  '\"' + recognized[i] + '\"')\n",
        "\n",
        "    # print validation result\n",
        "    char_error_rate = num_char_err / num_char_total\n",
        "    word_accuracy = num_word_ok / num_word_total\n",
        "    print(f'Character error rate: {char_error_rate * 100.0}%. Word accuracy: {word_accuracy * 100.0}%.')\n",
        "    return char_error_rate, word_accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9fVYj7Pag19"
      },
      "source": [
        "#### infer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOpPU3QTaKHM"
      },
      "source": [
        "def infer(model: Model, fn_img: Path) -> None:\n",
        "    \"\"\"Recognizes text in image provided by file path.\"\"\"\n",
        "    img = cv2.imread(fn_img, cv2.IMREAD_GRAYSCALE)\n",
        "    assert img is not None\n",
        "\n",
        "    preprocessor = Preprocessor(img_size, dynamic_width=True, padding=16)\n",
        "    img = preprocessor.process_img(img)\n",
        "\n",
        "    batch = Batch([img], None, 1)\n",
        "    recognized, probability = model.infer_batch(batch, True)\n",
        "    recognized_corrected = YandexSpeller().spelled(recognized[0])\n",
        "    print(f'Recognized: \"{recognized[0]}\"')\n",
        "    print(f'Corrected: \"{recognized_corrected}\"')\n",
        "    print(f'Probability: {probability[0]}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UzHAK0MftEY"
      },
      "source": [
        "def run_model(mode, decoder, early_stopping=10, fast=True, img_file=None):\n",
        "\n",
        "  char_list = list('абвгдежзийклмнопрстуфхцчшщъыьэюя ')\n",
        "  decoder_mapping = {'bestpath': DecoderType.BestPath,\n",
        "                            'beamsearch': DecoderType.BeamSearch,\n",
        "                            'wordbeamsearch': DecoderType.WordBeamSearch}\n",
        "  decoder_type = decoder_mapping[decoder]\n",
        "\n",
        "  if mode == 'train' or  mode == 'validate':\n",
        "\n",
        "          # load training data, create TF model\n",
        "          loader = DataLoader(drive_path, batch_size=500, fast=fast)\n",
        "\n",
        "          # save words contained in dataset into file\n",
        "          open(FilePaths.fn_corpus, 'w').write(' '.join(loader.train_texts + loader.validation_texts))\n",
        "\n",
        "          # execute training or validation\n",
        "          if mode == 'train':\n",
        "              model = Model(char_list, decoder_type)\n",
        "              train(model, loader, early_stopping=early_stopping)\n",
        "          elif mode == 'validate':\n",
        "              model = Model(char_list, decoder_type, must_restore=True)\n",
        "              validate(model, loader, early_stopping=early_stopping)\n",
        "          \n",
        "  # infer text on test image\n",
        "  elif mode == 'infer':\n",
        "      model = Model(char_list, decoder_type, must_restore=True)\n",
        "      infer(model, img_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0-JFZvuZ30p"
      },
      "source": [
        "### Запуск"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEMz4BI7yEKd",
        "collapsed": true
      },
      "source": [
        "run_model('train', 'wordbeamsearch')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yc-ukZL4h8PW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea881778-a571-46cb-bc4d-c384b917f570"
      },
      "source": [
        "# !nvidia-smi --query-gpu=gpu_name,driver_version,memory.total --format=csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name, driver_version, memory.total [MiB]\n",
            "Tesla K80, 460.32.03, 11441 MiB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u48L9GZ7ZRpl"
      },
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWRvgxSxZTpr"
      },
      "source": [
        "### Получение предобученной модели и файлов для распознавания"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMH4fguLZQ5N"
      },
      "source": [
        "model_path = Path(drive_path / 'model')\n",
        "if not Path('./model').exists():\n",
        "  shutil.copytree(model_path, 'model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kej4TmcSd-ex"
      },
      "source": [
        "img_path = Path(drive_path / 'inference_img')\n",
        "if not Path('./inference_img').exists():\n",
        "  shutil.copytree(img_path, 'inference_img')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_u-72jqZcJQ"
      },
      "source": [
        "### Запуск"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmMS0EQY8GZn"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLFzXihNZdpa",
        "outputId": "910edc39-3e1b-4671-cee4-85ec0d105aee"
      },
      "source": [
        "tf.compat.v1.reset_default_graph()\n",
        "run_model('infer', 'wordbeamsearch', img_file=Path('inference_img/6309.jpg'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.7.11 (default, Jul  3 2021, 18:01:19) \n",
            "[GCC 7.5.0]\n",
            "Tensorflow: 2.6.0\n",
            "Init with stored values from ./model/snapshot-76\n",
            "INFO:tensorflow:Restoring parameters from ./model/snapshot-76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./model/snapshot-76\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recognized: \"случайная проверки\"\n",
            "Corrected: \"случайная проверки\"\n",
            "Probability: 0.003577725263312459\n"
          ]
        }
      ]
    }
  ]
}